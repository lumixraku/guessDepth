import onnxruntime as ort
import numpy as np
from PIL import Image
import io
import os
import tempfile
from typing import Optional
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import FileResponse
import uvicorn

app = FastAPI(title="MoGe Model API", description="HTTP API for MoGe model inference")

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹ä¼šè¯å’Œè¾“å…¥ä¿¡æ¯
model_session = None
model_inputs_info = None

def download_model():
    """ä» Hugging Face ä¸‹è½½æ¨¡å‹æ–‡ä»¶"""
    model_url = "https://huggingface.co/Ruicheng/moge-2-vitb-normal-onnx/resolve/main/model.onnx"
    model_path = "./models/moge_model.onnx"

    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    if not os.path.exists(model_path):
        import requests
        print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
        response = requests.get(model_url)
        if response.status_code == 200:
            with open(model_path, 'wb') as f:
                f.write(response.content)
            print("æ¨¡å‹ä¸‹è½½å®Œæˆ")
        else:
            print(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
    else:
        print("æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨")

    return model_path

def load_model(model_path):
    """åŠ è½½ ONNX æ¨¡å‹"""
    try:
        # å°è¯•ä¸åŒçš„æ‰§è¡Œæä¾›è€…
        providers = ['CPUExecutionProvider']
        if ort.get_available_providers():
            print(f"å¯ç”¨çš„æ‰§è¡Œæä¾›è€…: {ort.get_available_providers()}")

        session = ort.InferenceSession(model_path, providers=providers)
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")

        print(f"è¾“å…¥ä¿¡æ¯:")
        inputs_info = {}
        for input_meta in session.get_inputs():
            print(f"  åç§°: {input_meta.name}")
            print(f"  å½¢çŠ¶: {input_meta.shape}")
            print(f"  ç±»å‹: {input_meta.type}")
            inputs_info[input_meta.name] = {
                'shape': input_meta.shape,
                'type': input_meta.type,
                'meta': input_meta
            }

        print(f"è¾“å‡ºä¿¡æ¯:")
        for output_meta in session.get_outputs():
            print(f"  åç§°: {output_meta.name}")
            print(f"  å½¢çŠ¶: {output_meta.shape}")
            print(f"  ç±»å‹: {output_meta.type}")

        return session, inputs_info
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def preprocess_image(image_data):
    """é¢„å¤„ç†è¾“å…¥å›¾åƒ"""
    try:
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        original_size = image.size
        print(f"åŸå§‹å›¾åƒå°ºå¯¸: {original_size}")

        # è°ƒæ•´å›¾åƒå¤§å°
        target_size = (518, 518)
        image = image.resize(target_size, Image.Resampling.LANCZOS)
        image_array = np.array(image).astype(np.float32) / 255.0
        image_array = np.transpose(image_array, (2, 0, 1))
        image_array = np.expand_dims(image_array, axis=0)

        print(f"é¢„å¤„ç†åå½¢çŠ¶: {image_array.shape}")
        return image_array, original_size
    except Exception as e:
        print(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
        return None, None

def create_num_tokens_input(value):
    """åˆ›å»º num_tokens è¾“å…¥"""
    # ä½¿ç”¨æœ€ç®€å•æœ‰æ•ˆçš„æ–¹æ³•
    return np.array(value, dtype=np.int64).reshape(())

def run_inference(session, image_array, inputs_info, num_tokens_value=1024):
    """è¿è¡Œæ¨¡å‹æ¨ç†"""
    try:
        num_tokens_input = create_num_tokens_input(num_tokens_value)
        
        inputs = {
            'image': image_array,
            'num_tokens': num_tokens_input
        }

        # æ˜¾ç¤ºè¾“å…¥ä¿¡æ¯
        for name, data in inputs.items():
            if hasattr(data, 'shape'):
                print(f"    {name}: å½¢çŠ¶={data.shape}, ç±»å‹={type(data)}, dtype={getattr(data, 'dtype', 'N/A')}")
            else:
                print(f"    {name}: å€¼={data}, ç±»å‹={type(data)}")

        # è¿è¡Œæ¨ç†
        print(f"    æ¨ç†ä¸­...")
        outputs = session.run(None, inputs)
        print(f"    âœ… æˆåŠŸï¼")
        return outputs

    except Exception as e:
        error_msg = str(e)[:100]
        print(f"    âŒ å¤±è´¥: {error_msg}")
        return None

def postprocess_output(outputs):
    """åå¤„ç†è¾“å‡ºç»“æœ"""
    if outputs is None or len(outputs) == 0:
        return None

    print(f"\nğŸ¯ è¾“å‡ºä¿¡æ¯:")
    for i, output in enumerate(outputs):
        print(f"  è¾“å‡º {i}: å½¢çŠ¶ {output.shape}, èŒƒå›´ {output.min():.6f} åˆ° {output.max():.6f}")

    # æ ¹æ®æ¨¡å‹è¾“å‡ºï¼Œnormal æ˜¯ç¬¬äºŒä¸ªè¾“å‡º
    if len(outputs) >= 2:
        normal_output = outputs[1]  # normal
        print(f"ä½¿ç”¨æ³•çº¿è¾“å‡º (outputs[1])ï¼Œå½¢çŠ¶: {normal_output.shape}")

        # å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
        if len(normal_output.shape) == 4:  # (1, H, W, C) æˆ– (1, C, H, W)
            normal_output = normal_output[0]  # ç§»é™¤ batch ç»´åº¦: (H, W, C) æˆ– (C, H, W)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬ç½®
            if normal_output.shape[0] == 3:  # (C, H, W) æ ¼å¼
                normal_output = np.transpose(normal_output, (1, 2, 0))  # CHW -> HWC
            elif normal_output.shape[1] == 3:  # (H, C, W) æ ¼å¼
                normal_output = np.transpose(normal_output, (0, 2, 1))  # HCW -> HWC
            # å¦‚æœæ˜¯ (H, W, C) æ ¼å¼ï¼Œä¸éœ€è¦è½¬ç½®

        print(f"åå¤„ç†åå½¢çŠ¶: {normal_output.shape}")
        return normal_output

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡º
    output = outputs[0]
    if len(output.shape) == 4:  # (1, H, W, C) æˆ– (1, C, H, W)
        output = output[0]
        if output.shape[0] == 3:  # (C, H, W)
            output = np.transpose(output, (1, 2, 0))  # CHW -> HWC
        elif output.shape[1] == 3:  # (H, C, W)
            output = np.transpose(output, (0, 2, 1))  # HCW -> HWC
    elif len(output.shape) == 3 and output.shape[0] == 1:  # (1, H, W)
        output = output[0]

    return output

def save_result_to_temp_file(result):
    """å°†ç»“æœä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›è·¯å¾„"""
    if result is None:
        return None

    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
        temp_path = temp_file.name
        temp_file.close()

        print(f"ğŸ’¾ ä¿å­˜ç»“æœï¼Œå½¢çŠ¶: {result.shape}")

        # å¦‚æœæ˜¯å•é€šé“ï¼ˆæ·±åº¦å›¾æˆ–é®ç½©ï¼‰
        if len(result.shape) == 2:
            # å½’ä¸€åŒ–åˆ° 0-255
            normalized = ((result - result.min()) / (result.max() - result.min()) * 255).astype(np.uint8)
            Image.fromarray(normalized, mode='L').save(temp_path)
            print(f"ä¿å­˜ä¸ºç°åº¦å›¾åƒ")

        # å¦‚æœæ˜¯ä¸‰é€šé“ï¼Œç¡®ä¿æ˜¯æ­£ç¡®çš„ HWC æ ¼å¼
        elif len(result.shape) == 3:
            if result.shape[2] == 3:  # (H, W, 3) - æ­£ç¡®æ ¼å¼
                # æ³•çº¿å›¾é€šå¸¸åœ¨ [-1, 1] èŒƒå›´å†…
                if result.min() < 0:
                    normalized = (result + 1) / 2  # [-1,1] -> [0,1]
                    print(f"æ³•çº¿å›¾èŒƒå›´è½¬æ¢: [{result.min():.3f}, {result.max():.3f}] -> [0, 1]")
                else:
                    normalized = np.clip(result, 0, 1)  # ç¡®ä¿åœ¨ [0,1] èŒƒå›´å†…

                result_8bit = (normalized * 255).astype(np.uint8)
                Image.fromarray(result_8bit).save(temp_path)
                print(f"ä¿å­˜ä¸ºRGBå›¾åƒ")

            elif result.shape[2] == 1:  # (H, W, 1) - å•é€šé“ä½†æœ‰é¢å¤–ç»´åº¦
                result_2d = result[:, :, 0]
                normalized = ((result_2d - result_2d.min()) / (result_2d.max() - result_2d.min()) * 255).astype(np.uint8)
                Image.fromarray(normalized, mode='L').save(temp_path)
                print(f"ä¿å­˜ä¸ºç°åº¦å›¾åƒï¼ˆç§»é™¤é€šé“ç»´åº¦ï¼‰")

            elif result.shape[0] == 3:  # (3, H, W) - éœ€è¦è½¬ç½®
                result_hwc = np.transpose(result, (1, 2, 0))
                if result_hwc.min() < 0:
                    normalized = (result_hwc + 1) / 2
                else:
                    normalized = np.clip(result_hwc, 0, 1)
                result_8bit = (normalized * 255).astype(np.uint8)
                Image.fromarray(result_8bit).save(temp_path)
                print(f"ä¿å­˜ä¸ºRGBå›¾åƒï¼ˆCHW->HWCè½¬æ¢ï¼‰")

            else:
                print(f"âš ï¸ æœªçŸ¥çš„3Då½¢çŠ¶: {result.shape}")
                # å°è¯•ä¿å­˜ç¬¬ä¸€ä¸ªé€šé“
                if result.shape[0] < result.shape[2]:  # å‡è®¾ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯é€šé“
                    first_channel = result[0, :, :]
                else:
                    first_channel = result[:, :, 0]
                normalized = ((first_channel - first_channel.min()) / (first_channel.max() - first_channel.min()) * 255).astype(np.uint8)
                Image.fromarray(normalized, mode='L').save(temp_path)
                print(f"ä¿å­˜ç¬¬ä¸€ä¸ªé€šé“ä¸ºç°åº¦å›¾åƒ")

        else:
            print(f"âš ï¸ å®Œå…¨æœªçŸ¥çš„å½¢çŠ¶: {result.shape}ï¼Œåªä¿å­˜åŸå§‹æ•°æ®")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºäº†
        if os.path.exists(temp_path):
            file_size = os.path.getsize(temp_path)
            print(f"âœ… å›¾åƒæˆåŠŸä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_path} (å¤§å°: {file_size} å­—èŠ‚)")
            return temp_path
        else:
            print(f"âŒ å›¾åƒæ–‡ä»¶æœªåˆ›å»º: {temp_path}")
            return None

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

@app.on_event("startup")
async def load_model_on_startup():
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model_session, model_inputs_info
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model_path = download_model()
    if model_path is None:
        print("æ¨¡å‹ä¸‹è½½å¤±è´¥")
        return
    
    model_session, model_inputs_info = load_model(model_path)
    if model_session is None:
        print("æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    print("æ¨¡å‹åŠ è½½å®Œæˆ")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å› API ä¿¡æ¯"""
    return {
        "message": "MoGe Model API",
        "description": "HTTP API for MoGe model inference",
        "endpoints": {
            "POST /process": "Process an image and return the result",
            "GET /": "This information page"
        }
    }

@app.post("/process")
async def process_image(
    image: UploadFile = File(...),
    num_tokens: Optional[int] = 1024
):
    """å¤„ç†ä¸Šä¼ çš„å›¾åƒå¹¶è¿”å›ç»“æœ"""
    global model_session, model_inputs_info
    
    if model_session is None:
        raise HTTPException(status_code=500, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è¯»å–ä¸Šä¼ çš„å›¾åƒ
        image_data = await image.read()
        
        # é¢„å¤„ç†å›¾åƒ
        image_array, original_size = preprocess_image(image_data)
        if image_array is None:
            raise HTTPException(status_code=400, detail="å›¾åƒå¤„ç†å¤±è´¥")
        
        # è¿è¡Œæ¨ç†
        outputs = run_inference(model_session, image_array, model_inputs_info, num_tokens)
        if outputs is None:
            raise HTTPException(status_code=500, detail="æ¨¡å‹æ¨ç†å¤±è´¥")
        
        # åå¤„ç†è¾“å‡º
        result = postprocess_output(outputs)
        if result is None:
            raise HTTPException(status_code=500, detail="ç»“æœå¤„ç†å¤±è´¥")
        
        # ä¿å­˜ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = save_result_to_temp_file(result)
        if temp_path is None:
            raise HTTPException(status_code=500, detail="ç»“æœä¿å­˜å¤±è´¥")
        
        # è¿”å›æ–‡ä»¶å“åº”
        return FileResponse(
            temp_path,
            media_type="image/png",
            filename="result.png",
            headers={"result-shape": str(result.shape)}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç†å›¾åƒæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)